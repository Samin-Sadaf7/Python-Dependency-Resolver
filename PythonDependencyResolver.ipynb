{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+5zvjyqpxCiK0cafgxiFY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samin-Sadaf7/Python-Dependency-Resolver/blob/main/PythonDependencyResolver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h0DJJJ5ZPeCj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f17b01-d655-4274-e71c-36658e7de6d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed faiss-cpu-1.10.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 faiss-cpu sentence-transformers openai numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Libraries\n",
        "import os\n",
        "import subprocess\n",
        "import glob\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from openai import OpenAI\n",
        "import re"
      ],
      "metadata": {
        "id": "h7mpWcozPj1G"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clone_repo(github_url):\n",
        "    \"\"\"\n",
        "    Clone the GitHub repository if not already cloned.\n",
        "    Returns the repository directory name.\n",
        "    \"\"\"\n",
        "    repo_name = github_url.rstrip('/').split('/')[-1]\n",
        "    if repo_name.endswith('.git'):\n",
        "        repo_name = repo_name[:-4]\n",
        "    if os.path.exists(repo_name):\n",
        "        print(f\"Repository '{repo_name}' already exists. Skipping clone.\")\n",
        "        return repo_name\n",
        "    print(f\"Cloning repository from {github_url} ...\")\n",
        "    cmd = [\"git\", \"clone\", github_url]\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(\"Error cloning repository:\", result.stderr)\n",
        "        return None\n",
        "    return repo_name\n",
        "\n",
        "\n",
        "def read_codebase(project_dir):\n",
        "    \"\"\"\n",
        "    Read all Python files from a project directory\n",
        "    \"\"\"\n",
        "    code = \"\"\n",
        "    for filepath in glob.glob(os.path.join(project_dir, '**', '*.py'), recursive=True):\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                code += f\"\\n# File: {filepath}\\n\" + f.read() + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {filepath}: {e}\")\n",
        "    return code\n",
        "\n",
        "\n",
        "def read_requirements(project_dir):\n",
        "    \"\"\"\n",
        "    Read requirements.txt from a project directory\n",
        "    \"\"\"\n",
        "    req_path = os.path.join(project_dir, 'requirements.txt')\n",
        "    if os.path.exists(req_path):\n",
        "        with open(req_path, 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "    return \"\""
      ],
      "metadata": {
        "id": "zNaKe2QFRvfL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_web(query):\n",
        "    \"\"\"\n",
        "    Perform targeted web searches for Python package documentation and compatibility information.\n",
        "    Returns relevant URLs based on detected packages/versions in the query.\n",
        "    \"\"\"\n",
        "    # Extract packages and versions using regex\n",
        "    package_pattern = r\"(\\b[\\w\\-]+\\b)(?:[=<>~!]=?|\\s+)?([\\d\\.\\*]+)?\"\n",
        "    matches = re.findall(package_pattern, query)\n",
        "\n",
        "    urls = []\n",
        "    for package, version in matches:\n",
        "        # Generate package-specific URLs\n",
        "        base_urls = [\n",
        "            f\"https://pypi.org/project/{package}/{version if version else ''}\",\n",
        "            f\"https://{package}.readthedocs.io/en/{version if version else 'latest'}/\",\n",
        "            f\"https://github.com/{package}/{package}/releases/tag/v{version}\"\n",
        "        ]\n",
        "\n",
        "        # Add version-specific documentation if available\n",
        "        if version and version != '*':\n",
        "            urls.extend([\n",
        "                f\"https://{package}.readthedocs.io/en/v{version}/\",\n",
        "                f\"https://www.versioneye.com/python/{package}/{version}\"\n",
        "            ])\n",
        "\n",
        "        urls.extend(base_urls)\n",
        "\n",
        "    # Add general Python packaging URLs\n",
        "    urls.extend([\n",
        "        \"https://pip.pypa.io/en/stable/\",\n",
        "        \"https://packaging.python.org/en/latest/\",\n",
        "        \"https://github.com/pypa/packaging-problems/issues\"\n",
        "    ])\n",
        "\n",
        "    return list(set(urls))  # Remove duplicates\n",
        "\n",
        "\n",
        "def collect_data_from_url(url):\n",
        "    \"\"\"\n",
        "    Fetch and extract relevant content from package-specific URLs.\n",
        "    Returns structured data with source information.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=15)\n",
        "        if response.status_code != 200:\n",
        "            return \"\"\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        content = \"\"\n",
        "\n",
        "        # PyPI-specific content extraction\n",
        "        if \"pypi.org\" in url:\n",
        "            main_content = soup.find('div', class_='project-description')\n",
        "            if main_content:\n",
        "                content += f\"PyPI Documentation ({url}):\\n\"\n",
        "                content += main_content.get_text(separator=\"\\n\", strip=True) + \"\\n\\n\"\n",
        "\n",
        "        # ReadTheDocs-specific content extraction\n",
        "        elif \"readthedocs.io\" in url:\n",
        "            article = soup.find('div', role='main')\n",
        "            if article:\n",
        "                content += f\"Official Documentation ({url}):\\n\"\n",
        "                content += article.get_text(separator=\"\\n\", strip=True) + \"\\n\\n\"\n",
        "\n",
        "        # GitHub Releases-specific content extraction\n",
        "        elif \"github.com\" in url and \"releases\" in url:\n",
        "            release_body = soup.find('div', class_='markdown-body')\n",
        "            if release_body:\n",
        "                content += f\"GitHub Release Notes ({url}):\\n\"\n",
        "                content += release_body.get_text(separator=\"\\n\", strip=True) + \"\\n\\n\"\n",
        "\n",
        "        # General fallback\n",
        "        else:\n",
        "            main_text = soup.get_text(separator=\"\\n\", strip=True)\n",
        "            content += f\"General Content ({url}):\\n{main_text}\\n\\n\"\n",
        "\n",
        "        return content.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url}: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "s6WoOLc0R1eH"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingModel:\n",
        "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.tokenizer = lambda text: text.split()  # Simple whitespace tokenizer\n",
        "\n",
        "    def encode(self, text):\n",
        "        return self.model.encode(text)\n",
        "\n",
        "    def count_tokens(self, text):\n",
        "        return len(self.tokenizer(text))\n",
        "\n",
        "\n",
        "class VectorDB:\n",
        "    def __init__(self, embedding_dim, max_tokens=2000):\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_tokens = max_tokens\n",
        "        self.index = faiss.IndexFlatL2(embedding_dim)\n",
        "        self.texts = []\n",
        "        self.token_counts = []\n",
        "\n",
        "    def add(self, embedding, text):\n",
        "        embedding = np.array(embedding).astype(\"float32\").reshape(1, -1)\n",
        "        self.index.add(embedding)\n",
        "        self.texts.append(text)\n",
        "        self.token_counts.append(len(text.split()))  # Simple token counting\n",
        "\n",
        "    def search(self, query_embedding, k=10):\n",
        "        query_embedding = np.array(query_embedding).astype(\"float32\").reshape(1, -1)\n",
        "        distances, indices = self.index.search(query_embedding, k)\n",
        "\n",
        "        results = []\n",
        "        total_tokens = 0\n",
        "        for i in indices[0]:\n",
        "            if i < len(self.texts) and (total_tokens + self.token_counts[i]) <= self.max_tokens:\n",
        "                results.append(self.texts[i])\n",
        "                total_tokens += self.token_counts[i]\n",
        "            elif total_tokens >= self.max_tokens:\n",
        "                break\n",
        "        return results\n",
        "\n",
        "\n",
        "def rerank_results(query_embedding, texts, embedding_model, max_tokens=2000):\n",
        "    \"\"\"\n",
        "    Re-rank texts with token-constrained selection\n",
        "    \"\"\"\n",
        "    scored_texts = []\n",
        "    for text in texts:\n",
        "        text_embedding = embedding_model.encode(text)\n",
        "        score = cosine_similarity(query_embedding, text_embedding)\n",
        "        token_count = embedding_model.count_tokens(text)\n",
        "        scored_texts.append((text, score, token_count))\n",
        "\n",
        "    # Sort by descending similarity score\n",
        "    scored_texts.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select texts within token budget\n",
        "    selected_texts = []\n",
        "    current_tokens = 0\n",
        "    for text, score, tokens in scored_texts:\n",
        "        if current_tokens + tokens <= max_tokens:\n",
        "            selected_texts.append(text)\n",
        "            current_tokens += tokens\n",
        "        else:\n",
        "            # Attempt to add partial content if remaining space\n",
        "            remaining_tokens = max_tokens - current_tokens\n",
        "            if remaining_tokens > 50:  # Only add chunks >50 tokens\n",
        "                truncated = \" \".join(text.split()[:remaining_tokens])\n",
        "                selected_texts.append(truncated + \"... [TRUNCATED]\")\n",
        "                current_tokens += remaining_tokens\n",
        "            break\n",
        "\n",
        "    return selected_texts"
      ],
      "metadata": {
        "id": "udwzGUFkR2WA"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dependency_status(context, codebase, package_versions, openai_api_key):\n",
        "    \"\"\"\n",
        "    Ask the LLM whether package version changes are required.\n",
        "    Expects an answer formatted as:\n",
        "      Package Version Changes Required: <Yes/No>\n",
        "      Explanation: <brief explanation>\n",
        "    \"\"\"\n",
        "    from openai import OpenAI\n",
        "\n",
        "    client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "    system_message = \"\"\"You are an expert Python dependency resolver. Analyze the given codebase and\n",
        "    requirements to determine if package version changes are needed to resolve dependency issues.\"\"\"\n",
        "\n",
        "    user_message = f\"\"\"\n",
        "    Relevant context from documentation and discussions:\n",
        "    {context}\n",
        "\n",
        "    Codebase:\n",
        "    {codebase}\n",
        "\n",
        "    Current package requirements:\n",
        "    {package_versions}\n",
        "\n",
        "    Answer in format:\n",
        "    Package Version Changes Required: <Yes/No>\n",
        "    Explanation: <brief explanation>\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ],\n",
        "        temperature=0.2,\n",
        "        max_tokens=150\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "def get_new_requirements(context, codebase, package_versions, openai_api_key):\n",
        "    \"\"\"\n",
        "    Ask the LLM to generate an updated requirements.txt file.\n",
        "    If no changes are needed, the LLM should output the original file.\n",
        "    \"\"\"\n",
        "    from openai import OpenAI\n",
        "\n",
        "    client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "    system_message = \"\"\"You are an expert Python dependency resolver. Generate an updated requirements.txt\n",
        "    that resolves dependency conflicts. If no changes needed, return the original content.\"\"\"\n",
        "\n",
        "    user_message = f\"\"\"\n",
        "    Relevant context from documentation and discussions:\n",
        "    {context}\n",
        "\n",
        "    Codebase:\n",
        "    {codebase}\n",
        "\n",
        "    Current package requirements:\n",
        "    {package_versions}\n",
        "\n",
        "    Output only valid requirements.txt contents:\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ],\n",
        "        temperature=0.2,\n",
        "        max_tokens=500\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "pNnO61lER8BO"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(github_url, openai_api_key):\n",
        "    # Clone the main repository\n",
        "    repo_dir = clone_repo(github_url)\n",
        "    if not repo_dir:\n",
        "        print(\"Failed to clone repository. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Path to Dataset directory\n",
        "    dataset_dir = os.path.join(repo_dir, \"Dataset\")\n",
        "    if not os.path.exists(dataset_dir):\n",
        "        print(\"Dataset directory not found. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Process each project in the Dataset directory\n",
        "    for project_name in os.listdir(dataset_dir):\n",
        "        project_dir = os.path.join(dataset_dir, project_name)\n",
        "\n",
        "        # Skip non-directories and files without \"project\" in name\n",
        "        if not os.path.isdir(project_dir) or \"project\" not in project_name.lower():\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n{'='*40}\")\n",
        "        print(f\"Processing project: {project_name}\")\n",
        "        print(f\"{'='*40}\")\n",
        "\n",
        "        # Read project contents\n",
        "        codebase = read_codebase(project_dir)\n",
        "        package_versions = read_requirements(project_dir)\n",
        "\n",
        "        if not codebase:\n",
        "            print(f\"No Python code found in {project_name}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Collect contextual information (existing implementation)\n",
        "        query = f\"Python dependency resolution best practices and package compatibility. Packages:{package_versions}\"\n",
        "        #print(f\"Query: {query}\")\n",
        "        urls = search_web(query)\n",
        "        collected_texts = [collect_data_from_url(url) for url in urls]\n",
        "        collected_texts = [text for text in collected_texts if text]\n",
        "\n",
        "        # Build context (existing implementation)\n",
        "        context = \"\"\n",
        "        if collected_texts:\n",
        "            embedding_model = EmbeddingModel()\n",
        "            embedding_dim = 384\n",
        "            vector_db = VectorDB(embedding_dim=embedding_dim)\n",
        "            for text in collected_texts:\n",
        "                emb = embedding_model.encode(text)\n",
        "                vector_db.add(emb, text)\n",
        "\n",
        "            query_embedding = embedding_model.encode(query)\n",
        "            search_results = vector_db.search(query_embedding, k=5)\n",
        "            reranked_results = rerank_results(query_embedding, search_results, embedding_model)\n",
        "            context = \"\\n\\n\".join(reranked_results)\n",
        "\n",
        "        #print(f\"context: {context}\")\n",
        "        #print(len(context))\n",
        "        # Get dependency status\n",
        "        status_response = get_dependency_status(context, codebase, package_versions, openai_api_key)\n",
        "        print(\"\\n=== Dependency Status ===\")\n",
        "        print(status_response)\n",
        "\n",
        "        # Generate new requirements\n",
        "        new_requirements = get_new_requirements(context, codebase, package_versions, openai_api_key)\n",
        "\n",
        "        # Save results per project\n",
        "        output_dir = os.path.join(repo_dir, \"generated_requirements\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        output_path = os.path.join(output_dir, f\"{project_name}_requirements.txt\")\n",
        "\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(new_requirements)\n",
        "\n",
        "        print(f\"\\n=== Generated requirements saved to: {output_path} ===\")\n",
        "\n",
        "    print(\"\\nProcessing completed for all projects in Dataset folder.\")"
      ],
      "metadata": {
        "id": "OzZuTdupSIMK"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OpenAI_API_KEY = userdata.get('OpenAIKey')"
      ],
      "metadata": {
        "id": "REOqg_GTSU-m"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_pipeline(\n",
        "    github_url=\"https://github.com/Samin-Sadaf7/Python-Dependency-Resolver.git\",\n",
        "    openai_api_key= OpenAI_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "8EZXWOanSfLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61c7eecc-f917-48a3-a02e-d767dcf89e96"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repository 'Python-Dependency-Resolver' already exists. Skipping clone.\n",
            "\n",
            "========================================\n",
            "Processing project: tensorflow_project\n",
            "========================================\n",
            "Query: Python dependency resolution best practices and package compatibility. Packages:tensorflow==2.5.0\n",
            "keras==3.0.0  \n",
            "Error processing https://www.versioneye.com/python/tensorflow/2.5.0: HTTPSConnectionPool(host='www.versioneye.com', port=443): Read timed out. (read timeout=15)\n",
            "\n",
            "=== Dependency Status ===\n",
            "Package Version Changes Required: Yes\n",
            "\n",
            "Explanation: The current requirement for Keras is version 3.0.0, which is affected by a security vulnerability (CVE-2024-55459) that allows attackers to write arbitrary files to the user's machine. It is recommended to update Keras to version 3.8.0, which is a secure version that addresses this vulnerability.\n",
            "\n",
            "=== Generated requirements saved to: Python-Dependency-Resolver/generated_requirements/tensorflow_project_requirements.txt ===\n",
            "\n",
            "========================================\n",
            "Processing project: pandas_project\n",
            "========================================\n",
            "Query: Python dependency resolution best practices and package compatibility. Packages:pandas==1.3.0\n",
            "numpy==2.0.0 \n",
            "\n",
            "=== Dependency Status ===\n",
            "Package Version Changes Required: Yes\n",
            "\n",
            "Explanation: The current package requirements specify pandas version 1.3.0 and numpy version 2.0.0. However, the codebase imports numpy and pandas to perform data analysis. The codebase uses numpy's random function to generate random data for a pandas DataFrame. Since the codebase relies on numpy functionality, and the latest available numpy version is 2.2.2, it is recommended to update the numpy version to at least 2.2.2 to ensure compatibility and potentially leverage any improvements or bug fixes in the newer version.\n",
            "\n",
            "=== Generated requirements saved to: Python-Dependency-Resolver/generated_requirements/pandas_project_requirements.txt ===\n",
            "\n",
            "========================================\n",
            "Processing project: requests_project\n",
            "========================================\n",
            "Query: Python dependency resolution best practices and package compatibility. Packages:requests==2.26.0\n",
            "urllib3==2.0.0 \n",
            "\n",
            "=== Dependency Status ===\n",
            "Package Version Changes Required: Yes\n",
            "\n",
            "Explanation: The current version of urllib3 being used (2.0.0) has known security vulnerabilities as mentioned in the provided documentation. It is recommended to update urllib3 to version 2.3.0 to address these vulnerabilities and ensure the security of the application.\n",
            "\n",
            "=== Generated requirements saved to: Python-Dependency-Resolver/generated_requirements/requests_project_requirements.txt ===\n",
            "\n",
            "========================================\n",
            "Processing project: matplotlib_project\n",
            "========================================\n",
            "Query: Python dependency resolution best practices and package compatibility. Packages:matplotlib==3.4.0\n",
            "numpy==2.0.0  \n",
            "\n",
            "=== Dependency Status ===\n",
            "Package Version Changes Required: No\n",
            "\n",
            "Explanation: The current package requirements for matplotlib and numpy are already specified as matplotlib==3.4.0 and numpy==2.0.0 respectively. These versions are compatible with the codebase provided in the plot.py script, so no version changes are needed to resolve dependency issues.\n",
            "\n",
            "=== Generated requirements saved to: Python-Dependency-Resolver/generated_requirements/matplotlib_project_requirements.txt ===\n",
            "\n",
            "Processing completed for all projects in Dataset folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kovYZ19zhjDx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}